{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country', 'year', 'Life Ladder', 'Log GDP per capita',\n",
      "       'Social support', 'Healthy life expectancy at birth',\n",
      "       'Freedom to make life choices', 'Generosity',\n",
      "       'Perceptions of corruption', 'Positive affect', 'Negative affect',\n",
      "       'Confidence in national government', 'Democratic Quality',\n",
      "       'Delivery Quality', 'Standard deviation of ladder by country-year',\n",
      "       'Standard deviation/Mean of ladder by country-year',\n",
      "       'GINI index (World Bank estimate)',\n",
      "       'GINI index (World Bank estimate), average 2000-15',\n",
      "       'gini of household income reported in Gallup, by wp5-year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(WHRDataSet_filename)\n",
    "df.head()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The data set we have chosen is the World Happiness Report\n",
    "2. We will be predicting healthy life expectancy at birth and that is also the label\n",
    "3. This is a supervised linear regression problem. This is not a classification problem.\n",
    "4. Our features will be features besides the label and any with >100 NaN values\n",
    "6. This is an important problem to explore because it can help us understand where we need to allocate more of our health resources to increase the life expectancy in that particular country. If we know where the problem is, then we can better understand how to target and solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. We will address missing values by removing them, as well as finding and replacing outliers. We will also combine data from the same countries to make it more efficient. We will remove irrelevant features.\n",
    "#2. We will use a logistic regression model to analyze our data. We will graph the difference between our testing and training results to make sure that it isn't overfitting. We will also change hyperparameters to see which works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?Â \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We plan to see how many values are missing when cleaning our data and decide our top 5 features from there. Our model is a logistic regression model. We plan to train our model and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       country  Life Ladder  Log GDP per capita  Social support  \\\n",
      "0  Afghanistan     3.806614            7.419697        0.517146   \n",
      "1      Albania     4.988791            9.247059        0.723204   \n",
      "2      Algeria     5.555004            9.501728        0.805639   \n",
      "3       Angola     4.420299            8.713935        0.737973   \n",
      "4    Argentina     6.406131            9.826051        0.906080   \n",
      "\n",
      "   Healthy life expectancy at birth  Freedom to make life choices  Generosity  \\\n",
      "0                         50.838271                      0.544895    0.118428   \n",
      "1                         68.027213                      0.626155   -0.105019   \n",
      "2                         64.984461                      0.600590   -0.138797   \n",
      "3                         51.729801                      0.455957   -0.077940   \n",
      "4                         66.764205                      0.753122   -0.154544   \n",
      "\n",
      "   Perceptions of corruption  Positive affect  Negative affect  ...  \\\n",
      "0                   0.826794         0.580873         0.301283  ...   \n",
      "1                   0.859691         0.642628         0.303256  ...   \n",
      "2                   0.692192         0.631932         0.265079  ...   \n",
      "3                   0.867018         0.613339         0.351173  ...   \n",
      "4                   0.844038         0.840998         0.273187  ...   \n",
      "\n",
      "   GINI index (World Bank estimate)  \\\n",
      "0                               NaN   \n",
      "1                          0.290000   \n",
      "2                          0.276000   \n",
      "3                               NaN   \n",
      "4                          0.447667   \n",
      "\n",
      "   GINI index (World Bank estimate), average 2000-15  \\\n",
      "0                                                NaN   \n",
      "1                                           0.303250   \n",
      "2                                           0.276000   \n",
      "3                                           0.427000   \n",
      "4                                           0.476067   \n",
      "\n",
      "   gini of household income reported in Gallup, by wp5-year  GDP_na  \\\n",
      "0                                           0.385668            0.0   \n",
      "1                                           0.492998            0.0   \n",
      "2                                           0.491331            0.0   \n",
      "3                                           0.514382            0.0   \n",
      "4                                           0.349463            0.0   \n",
      "\n",
      "   Social_support_na  Life_choices_na  Generosity_na  Corruption_na  \\\n",
      "0           0.000000         0.000000       0.000000       0.000000   \n",
      "1           0.000000         0.000000       0.000000       0.000000   \n",
      "2           0.166667         0.333333       0.333333       0.333333   \n",
      "3           0.000000         0.000000       0.000000       0.000000   \n",
      "4           0.000000         0.000000       0.000000       0.000000   \n",
      "\n",
      "   Positive_affect_na  Negative_affect_na  \n",
      "0            0.000000            0.000000  \n",
      "1            0.000000            0.000000  \n",
      "2            0.166667            0.166667  \n",
      "3            0.000000            0.000000  \n",
      "4            0.000000            0.000000  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "       country  Life Ladder  Log GDP per capita  Social support  \\\n",
      "0  Afghanistan     3.723590            7.168690        0.450662   \n",
      "1  Afghanistan     4.401778            7.333790        0.552308   \n",
      "2  Afghanistan     4.758381            7.386629        0.539075   \n",
      "3  Afghanistan     3.831719            7.415019        0.521104   \n",
      "4  Afghanistan     3.782938            7.517126        0.520637   \n",
      "\n",
      "   Healthy life expectancy at birth  Freedom to make life choices  Generosity  \\\n",
      "0                         49.209663                      0.718114    0.181819   \n",
      "1                         49.624432                      0.678896    0.203614   \n",
      "2                         50.008961                      0.600127    0.137630   \n",
      "3                         50.367298                      0.495901    0.175329   \n",
      "4                         50.709263                      0.530935    0.247159   \n",
      "\n",
      "   Perceptions of corruption  Positive affect  Negative affect  ...  \\\n",
      "0                   0.881686         0.517637         0.258195  ...   \n",
      "1                   0.850035         0.583926         0.237092  ...   \n",
      "2                   0.706766         0.618265         0.275324  ...   \n",
      "3                   0.731109         0.611387         0.267175  ...   \n",
      "4                   0.775620         0.710385         0.267919  ...   \n",
      "\n",
      "   GINI index (World Bank estimate)  \\\n",
      "0                               NaN   \n",
      "1                               NaN   \n",
      "2                               NaN   \n",
      "3                               NaN   \n",
      "4                               NaN   \n",
      "\n",
      "   GINI index (World Bank estimate), average 2000-15  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "   gini of household income reported in Gallup, by wp5-year  GDP_na  \\\n",
      "0                                                NaN          False   \n",
      "1                                           0.441906          False   \n",
      "2                                           0.327318          False   \n",
      "3                                           0.336764          False   \n",
      "4                                           0.344540          False   \n",
      "\n",
      "   Social_support_na  Life_choices_na  Generosity_na  Corruption_na  \\\n",
      "0              False            False          False          False   \n",
      "1              False            False          False          False   \n",
      "2              False            False          False          False   \n",
      "3              False            False          False          False   \n",
      "4              False            False          False          False   \n",
      "\n",
      "   Positive_affect_na  Negative_affect_na  \n",
      "0               False               False  \n",
      "1               False               False  \n",
      "2               False               False  \n",
      "3               False               False  \n",
      "4               False               False  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 85\u001b[0m\n\u001b[1;32m     80\u001b[0m y \u001b[38;5;241m=\u001b[39m df_model[label]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#Outlier removal\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#Fit your best model on the full dataset\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[43mgrid_search\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#Predict on the full dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "#Making a feature list with all the features\n",
    "feature_list = df.columns.tolist()\n",
    "feature_list.remove('Healthy life expectancy at birth')\n",
    "\n",
    "#HANDLING MISSING DATA\n",
    "#Determining which features have >100 NaN values \n",
    "nan_count = np.sum(df.isnull(), axis = 0)\n",
    "nan_count\n",
    "\n",
    "#Removing those features\n",
    "missing = df.isnull().sum()\n",
    "threshold = 100\n",
    "clean_features = [col for col in feature_list if missing[col] <= threshold]\n",
    "df_cleaned = df[clean_features]\n",
    "clean_features\n",
    "\n",
    "#Creating new columns \n",
    "df['GDP_na'] = df['Log GDP per capita'].isnull()\n",
    "df['Social_support_na'] = df['Social support'].isnull()\n",
    "df['Life_choices_na'] = df['Freedom to make life choices'].isnull()\n",
    "df['Generosity_na'] = df['Generosity'].isnull()\n",
    "df['Corruption_na'] = df['Perceptions of corruption'].isnull()\n",
    "df['Positive_affect_na'] = df['Positive affect'].isnull()\n",
    "df['Negative_affect_na'] = df['Negative affect'].isnull()\n",
    "\n",
    "#Fill in with mean values\n",
    "mean_GDP = df['Log GDP per capita'].mean()\n",
    "df['Log GDP per capita'] = df['Log GDP per capita'].fillna(mean_GDP)\n",
    "\n",
    "mean_support = df['Social support'].mean()\n",
    "df['Social support'] = df['Social support'].fillna(mean_support)\n",
    "\n",
    "mean_lifechoices = df['Freedom to make life choices'].mean()\n",
    "df['Freedom to make life choices'] = df['Freedom to make life choices'].fillna(mean_lifechoices)\n",
    "\n",
    "mean_generosity = df['Generosity'].mean()\n",
    "df['Generosity'] = df['Generosity'].fillna(mean_generosity)\n",
    "\n",
    "mean_perceptions = df['Perceptions of corruption'].mean()\n",
    "df['Perceptions of corruption'] = df['Perceptions of corruption'].fillna(mean_perceptions)\n",
    "\n",
    "mean_positive = df['Positive affect'].mean()\n",
    "df['Positive affect'] = df['Positive affect'].fillna(mean_positive)\n",
    "\n",
    "mean_negative = df['Negative affect'].mean()\n",
    "df['Negative affect'] = df['Negative affect'].fillna(mean_negative)\n",
    "\n",
    "#Removing year column and averaging values of countries through the years\n",
    "if 'year' in df.columns:\n",
    "    df = df.drop(columns=['year'], errors='ignore')\n",
    "\n",
    "df_avg = df. groupby ('country'). mean (numeric_only=True). reset_index()\n",
    "print(df_avg.head () )\n",
    "print(df. head ( ))\n",
    "\n",
    "#Dropping unneeded columns\n",
    "cols_to_drop = [\n",
    "    'GINI index (World Bank estimate)',\n",
    "    'GINI index (World Bank estimate), average 2000-15',\n",
    "    'Confidence in national government',\n",
    "    'gini of household income reported in Gallup, by wp5-year',\n",
    "    'Delivery Quality',\n",
    "    'Democratic Quality',\n",
    "]\n",
    "df_avg = df_avg.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "#DEFINING FEATURES AND LABEL\n",
    "\n",
    "label = 'Healthy life expectancy at birth'\n",
    "\n",
    "#Remove country column\n",
    "df_model = df_avg.drop(columns=['country'], errors='ignore')\n",
    "\n",
    "#Drop rows where target is NaN\n",
    "df_model = df_model.dropna(subset=[label])\n",
    "\n",
    "#Define X and y\n",
    "X = df_model.drop(columns=[label])\n",
    "y = df_model[label]\n",
    "\n",
    "#Outlier removal\n",
    "\n",
    "#Fit your best model on the full dataset\n",
    "model = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "model.fit(X, y)\n",
    "\n",
    "#Predict on the full dataset\n",
    "y_pred_all = model.predict(X)\n",
    "\n",
    "#Calculate residuals and standardized residuals\n",
    "residuals = y - y_pred_all\n",
    "std_residuals = (residuals - np.mean(residuals)) / np.std(residuals)\n",
    "\n",
    "#Set a threshold for outliers\n",
    "threshold = 3 \n",
    "outlier_mask = np.abs(std_residuals) > threshold\n",
    "\n",
    "#Report outliers\n",
    "print(f\"Number of outliers: {np.sum(outlier_mask)}\")\n",
    "\n",
    "#Filter them out\n",
    "X_no_outliers = X[~outlier_mask]\n",
    "y_no_outliers = y[~outlier_mask]\n",
    "\n",
    "\n",
    "#Splitting in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_no_outliers, y_no_outliers, test_size=0.25, random_state=123)\n",
    "\n",
    "#TRAINING LINEAR REGRESSION MODEL\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#EVALUATE THE MODEL\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.2f}\")\n",
    "\n",
    "#Get predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "#Plotting\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "#Training set plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.7, color='blue')\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "plt.xlabel('Actual (Train)')\n",
    "plt.ylabel('Predicted (Train)')\n",
    "plt.title('Training Set: Actual vs Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "#Test set plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.7, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual (Test)')\n",
    "plt.ylabel('Predicted (Test)')\n",
    "plt.title('Test Set: Actual vs Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(f\"Average CV RÂ² score: {scores.mean():.4f}\")\n",
    "\n",
    "#Performing grid search \n",
    "#Define ridge model\n",
    "ridge = Ridge(random_state=123)\n",
    "\n",
    "#Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100, 200]\n",
    "}\n",
    "\n",
    "#GridSearchCV with 5-fold cross-validation and RÂ² scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ridge,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#View best parameters and best score\n",
    "print(\"Best alpha:\", grid_search.best_params_['alpha'])\n",
    "print(f\"Best cross-validated RÂ²: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "#Use the best estimator to predict and evaluate on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RÂ²: {r2:.4f}\")\n",
    "\n",
    "#Get feature names and coefficients from the best Ridge model\n",
    "feature_names = X.columns\n",
    "coefs = best_model.coef_\n",
    "\n",
    "#Create a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefs\n",
    "})\n",
    "\n",
    "\n",
    "#Sort by absolute value of coefficients \n",
    "coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "coef_df_sorted = coef_df.sort_values(by='Abs_Coefficient', ascending=True)\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(coef_df_sorted['Feature'], coef_df_sorted['Coefficient'], color='skyblue')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.title('Feature Importance (Coefficient Values from Ridge Regression)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#CLEAN THE DATA MORE\n",
    "#Remove features where coefficient is exactly zero\n",
    "features_to_keep = coef_df[coef_df['Coefficient'] != 0]['Feature'].tolist()\n",
    "print(\"features to keep: \")\n",
    "print(features_to_keep)\n",
    "\n",
    "#Filter dataset to only keep those features\n",
    "X_filtered = X[features_to_keep]\n",
    "\n",
    "X_train_filtered, X_test_filtered, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=123)\n",
    "\n",
    "model = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "model.fit(X_train_filtered, y_train)\n",
    "\n",
    "#Evaluate the filtered model\n",
    "y_pred = model.predict(X_test_filtered)\n",
    "print(\"Filtered Test RÂ²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "#Convert column lists to sets \n",
    "original_features = set(X.columns)\n",
    "filtered_features = set(X_filtered.columns)\n",
    "\n",
    "#Find dropped columns\n",
    "dropped_features = original_features - filtered_features\n",
    "\n",
    "#Print the results\n",
    "if dropped_features:\n",
    "    print(\"Dropped features:\")\n",
    "    for feature in dropped_features:\n",
    "        print(\"-\", feature)\n",
    "else:\n",
    "    print(\"No features were dropped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
